---
filename: 50-open-source-options-for-running-llms-locally
category: Technology, Machine Learning
url: https://medium.com/thedeephub/50-open-source-options-for-running-llms-locally-db1ec6f5a54f
title: 50+ Open-Source Options for Running LLMs Locally | by Vince Lam | The Deep Hub | Mar, 2024 | Medium
description: A comprehensive list of open-source options for running Large Language Models (LLMs) locally.
---
# 50+ Open-Source Options for Running LLMs Locally

https://medium.com/thedeephub/50-open-source-options-for-running-llms-locally-db1ec6f5a54f

## Description

A comprehensive list of open-source options for running Large Language Models (LLMs) locally.

## Summary

In this article, the author discusses the benefits of using locally hosted open weights LLMs, such as data privacy and cost savings. By utilizing free models and occasionally switching to GPT-4, significant savings were achieved. The post is divided into sections covering all-in-one desktop solutions, LLM inference via the CLI and backend API servers, and front-end UIs for connecting to LLM backends.

Various open-source tools and repositories for hosting open weights LLMs locally are highlighted, including desktop solutions like GPT4All and LM Studio alternatives like Jaan. CLI tools like llama.cpp and Ollama are discussed for local inference, along with front-end UIs such as Open WebUI and Lobe Chat for enhanced user interactions. The article provides insights into each tool's features and recommendations based on user experience and community engagement.

Overall, the author recommends using tools like Ollama and Jan for local LLM inference, depending on user preferences. The article serves as a valuable resource for individuals looking to explore AI technologies locally and optimize their LLM usage for various tasks.
